---
title: "RidgeRegression"
author: "Philipp Grafendorfer"
date: "11 Januar 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
train <- readRDS(file = "data/train_preprocessed.rds")
validation <- readRDS(file = "data/validation_preprocessed.rds")
```

```{r}
set.seed(42)
library(glmnet)
library(psych)
library(dplyr)
library(purrr)
library(data.table)
```

```{r}
df_train <- data.table(train, keep.rownames = F)
sparse_matrix_train <- sparse.model.matrix(SalePrice ~ . -1 , data = df_train)

output_vector_train <- df_train[,SalePrice]
```


```{r}
# y <- train %>% 
#   select(SalePrice) %>% 
#   scale(center = TRUE, scale = FALSE) %>% 
#   as.matrix()
# X <- train %>% 
#   # dplyr::select_if(is.integer) %>% 
#   select(-SalePrice)
# X <- sparse.model.matrix(.~., data=X)

output_vector_train <- output_vector_train %>% 
  scale(center = TRUE, scale = FALSE) %>% 
  as.matrix()
```

```{r}
lambdas_to_try <- 10^seq(2, 8, length.out = 100)
# Setting alpha = 0 implements ridge regression
ridge_cv <- cv.glmnet(sparse_matrix_train, output_vector_train, alpha = 0, lambda = lambdas_to_try, standardize = TRUE, nfolds = 10)
# Plot cross-validation results
plot(ridge_cv)
```

```{r}
# Best cross-validated lambda
lambda_cv <- ridge_cv$lambda.min
# Fit final model, get its sum of squared residuals and multiple R-squared
model_cv <- glmnet(sparse_matrix_train, output_vector_train, alpha = 0, lambda = lambda_cv, standardize = TRUE)
y_hat_cv <- predict(model_cv, sparse_matrix_train)
ssr_cv <- t(output_vector_train - y_hat_cv) %*% (output_vector_train - y_hat_cv)
rsq_ridge_cv <- cor(output_vector_train, y_hat_cv)^2
```

