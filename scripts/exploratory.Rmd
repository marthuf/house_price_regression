---
title: 'Exporatory analysis of the Ames house price dataset'
output: html_document
---

```{r, message=F, comment=NA}
require(dplyr)
require(ggplot2)
require(tidyr)

train <- read.csv('../data/train.csv')
test <- read.csv('../data/test.csv')

cat('dimensions: ', dim(train))
head(train)
```

```{r}
available_test <- sapply(test, function(col) 1 - mean(is.na(col)))
available_train <- sapply(select(train, -SalePrice), function(col) 1 - mean(is.na(col)))
na_stats <- data.frame(available_test, available_train)
na_stats['variable'] <- row.names(na_stats)
row.names(na_stats) <- NULL
na_stats <- na_stats %>%
  mutate( available_min = pmin(available_train, available_test) ) %>% 
  mutate( complete = as.factor(available_min == 1) )
na_stats
```


```{r, fig.height=10}
fraction_available <- sapply(test, function(col) 1 - mean(is.na(col)))
available_values <- data.frame(fraction_available)
available_values['variable'] <- row.names(available_values)
row.names(available_values) <- NULL
available_values['complete'] <- as.factor(available_values$fraction_available == 1)

ggplot(available_values, aes(x=variable, y=fraction_available, fill=complete) ) + 
  geom_bar( stat='identity' ) + 
  coord_flip() + 
  scale_fill_manual(values=c('gray50', 'gray80')) +
  ylab('fraction of values available') +
  ggtitle('Fraction of available (not NA) values') + 
  theme(plot.title=element_text(size=16, hjust=0.5))
```
## feature analysis (and some engineering)

Check datatype of features. All features are either factor or numeric (has been checked). Some features with datatype 'numeric' may in reality be ordinal and it makes more sense to encode them as factor. If a feature has datatype numeric but only a few distinct values, that's a sign that it should maybe be used as a factor.
```{r}
dtypes <- data.frame(variable=colnames(test),
                     numeric=as.integer(sapply(test, is.numeric) ),
                     factor=as.integer(sapply(test, is.factor) ),
                     unique_values=sapply(test, function(x) length(unique(x)) ),
                     missing_values=sapply(test, function(x) mean(is.na(x)) )
)

# check for numeric columns that maybe should be factors:
filter(dtypes, numeric == T & unique_values <= 10)
```
```{r}
feature_engineer <- function(df) {
  # * make some new features and remove the old ones
  # * change some ordinal features from numeric to factor
  
  # most houses have no pool (area = 0) => only consider if a house has a pool or not:
  df['HasPool'] <- as.factor(df$PoolArea > 0)
  df$PoolArea <- NULL
  
  convert_to_factor <- c('BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr',
                         'KitchenAbvGr', 'Fireplaces', 'GarageCars')
  df[, convert_to_factor] <- lapply(df[, convert_to_factor], as.factor)
  
  current_year <- 2011  # year the dataset was built
  
  df['Age'] <- current_year - df$YearBuilt
  df['YearsSinceSale'] <- current_year - df$YrSold
  df[, c('MoSold', 'YrSold', 'YearBuilt')] <- NULL
  
  return(df)
}

train <- feature_engineer(train)
test <- feature_engineer(test)
```

Possible improvement: For all features which are mostly 0 but may have a numeric value, make a factor indiciating if it's there (i.e. not 0). For instance: EnclosedPorch. Most houses don't have it and for the rest, the feature provides an area.

## Missing value imputation
in progress...
```{r}
# variables with only a few missing values:
impute_na <- function(df) {
  
  notna_threshold <- 0.9  # drop feature if less than 90% available values
  impute_cols <- filter(available_values, fraction_available >= notna_threshold & complete == F)$variable

  factor_cols <- Filter(function(x) is.factor(df[[x]]), impute_cols)
  df[, factor_cols] <- lapply(df[, factor_cols], addNA)  # add NA as a factor level
  cat('\n\nset NA to factor-level for these columns:\n', factor_cols)
  
  numeric_cols <- Filter(function(col) is.numeric(df[[col]]), impute_cols)
  df[, numeric_cols] <- lapply(df[, numeric_cols], function(col) replace_na(col, median(col, na.rm=T)))
  cat('\n\nset NA to median value for these columns:\n', numeric_cols)

  remove_cols <- filter(available_values, fraction_available < notna_threshold)$variable
  df <- select(df, -remove_cols)
  cat('\n\nremoved these columns (too many NA-values):\n', remove_cols)
  
  return(df)
}

test_complete <- impute_na(test)


```


## Split training set in to validation-set and training set

```{r}
train <- train[, colSums(is.na(train)) == 0]  # remove all columns with NAs


validation_fraction <- 0.2
validation_row_indices <- sample(row.names(train), size=validation_fraction*nrow(train), replace=F)
train_row_indices = row.names(train)[!row.names(train) %in% validation_row_indices]

validation <- train[validation_row_indices, ]
train <- train[train_row_indices, ]
```

```{r}
naive_model <- lm(data=train, SalePrice ~ .)
summary(naive_model)
```

```{r}
#predict(naive_model, newdata=validation)
```



## ignore test set for now
```{r, echo=F, eval=F}
# test_complete <- test[, colSums(is.na(test)) == 0]  # remove all columns with NAs
# 
# # use all variables that are complete in the test set + the target variable (price)
# train_complete <- train[, c(colnames(test_complete), 'SalePrice')]
# 
# naive_model <- lm(data=train_complete, SalePrice ~ .)
# summary(naive_model)
```


